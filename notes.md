Notes for learning SGD:

- SGD updates parameters using mini-batches to approximate gradients.
- Key hyperparameters: learning rate (lr), batch size, momentum.
- Start with a simple dataset and small model to observe behavior.
- Try varying `lr` by an order of magnitude to see stability vs speed trade-offs.

Commands to run:

```
python examples/pytorch_sgd.py
```
